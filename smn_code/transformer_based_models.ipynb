{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758e85ef",
   "metadata": {},
   "source": [
    "# Transformer Based Models and Prediction\n",
    "\n",
    "This file contains code to create transformer based models and do predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, pipeline\n",
    "import evaluate\n",
    "import math\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_training_and_evaluation import *\n",
    "from model_testing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0994be6",
   "metadata": {},
   "source": [
    "## Create datasets\n",
    "\n",
    "Because of time-contraints, the models will only be tested with one set of parameters, and the final result of each BERT model will be compared to eachother. However no evaluation will be done to tune the parameters. Therefore, a validation dataset will not be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f44d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and split into train and test dataset. pred_400 is the final labeled dataset from the AL process\n",
    "full_df = pd.read_csv('./temp_pred_during_al/pred_400.csv')\n",
    "\n",
    "# Create train and test dataframes\n",
    "train_df, test_df = train_test_split(full_df, test_size=0.20, random_state=1, stratify=full_df[['label']])\n",
    "\n",
    "# Create balanced train dataset\n",
    "train_neg = train_df[train_df['label'] == 'LABEL_0']\n",
    "train_pos = train_df[train_df['label'] == 'LABEL_1'].sample(n=len(train_neg), random_state=0)\n",
    "train = pd.concat([train_neg, train_pos])\n",
    "\n",
    "train.loc[train['label'] == 'LABEL_0', 'label'] = 0\n",
    "train.loc[train['label'] == 'LABEL_1', 'label'] = 1\n",
    "\n",
    "# Create eval dataframe\n",
    "test, evaluate = train_test_split(test_df, test_size=0.20, random_state=1, stratify=test_df[['label']])\n",
    "\n",
    "evaluate.loc[evaluate['label'] == 'LABEL_0', 'label'] = 0\n",
    "evaluate.loc[evaluate['label'] == 'LABEL_1', 'label'] = 1\n",
    "\n",
    "# For huggingface transformers, the pandas dataframe must be turned into a huggingface dataset\n",
    "train_ds = Dataset.from_pandas(train)\n",
    "print(train_ds)\n",
    "\n",
    "eval_ds = Dataset.from_pandas(evaluate)\n",
    "print(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bce84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./emails_train_balanced.csv')\n",
    "test.to_csv('./emails_test.csv')\n",
    "evaluate.to_csv('./emails_eval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b59ad0",
   "metadata": {},
   "source": [
    "## Train models and make predictions\n",
    "\n",
    "Now 4 different BERT models will be trained and then used to make predictions on the test dataset. I will use the same parameters that gave the best results on the NoReC dataset, because I don't have time to fine-tuned the parameters on this computer's CPU. I will run all models for 5 epocsh and use the best one for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb83db",
   "metadata": {},
   "source": [
    "## distilmBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f076b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/distilmBERT-original')\n",
    "distilmbert_train_encoding = train_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "distilmbert_eval_encoding = eval_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Train model for 5 epochs\n",
    "fine_tuned_distilmbert_dir, info_from_distilmbert_training = create_and_train_model(\n",
    "    model_path='./models/distilmBERT-original',\n",
    "    model_name='distilmbert',\n",
    "    training_data=distilmbert_train_encoding,\n",
    "    eval_data=distilmbert_eval_encoding,\n",
    "    epochs=5,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_ratio=0.01,\n",
    "    optimizer='adamw_hf',\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "print(f'info_from_distilmbert_training: {info_from_distilmbert_training}')\n",
    "\n",
    "# Make predictions and check prediction time\n",
    "start_time = time.time()\n",
    "distilmbert_results = predict_from_fine_tuned_model(fine_tuned_distilmbert_dir, list(test['text']))\n",
    "print(f\"Time used for prediction was: {time.time() - start_time}\")\n",
    "\n",
    "# Because report_evaluation expects the actual labels to be a list of 0 and 1, this is neccessary:\n",
    "test.loc[test['label'] == 'LABEL_0', 'label'] = 0\n",
    "test.loc[test['label'] == 'LABEL_1', 'label'] = 1\n",
    "\n",
    "# Get scores/results\n",
    "report_evaluation(distilmbert_results['label'], list(test['label']), 'distilmbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf140c",
   "metadata": {},
   "source": [
    "## NB-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/NB-BERT-original')\n",
    "nb_bert_train_encoding = train_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "nb_bert_eval_encoding = eval_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Train model for 5 epochs\n",
    "fine_tuned_nb_bert_dir, info_from_nb_bert_training = create_and_train_model(\n",
    "    model_path='./models/NB-BERT-original',\n",
    "    model_name='nb-bert',\n",
    "    training_data=nb_bert_train_encoding,\n",
    "    eval_data=nb_bert_eval_encoding,\n",
    "    epochs=5,\n",
    "    learning_rate=5e-05,\n",
    "    warmup_ratio=0.1,\n",
    "    optimizer='adamw_hf',\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "print(f'info_from_distilmbert_training: {info_from_nb_bert_training}')\n",
    "\n",
    "# Make predictions and check prediction time\n",
    "start_time = time.time()\n",
    "nb_bert_results = predict_from_fine_tuned_model(fine_tuned_nb_bert_dir, list(test['text']))\n",
    "print(f\"Time used for prediction was: {time.time() - start_time}\")\n",
    "\n",
    "# Because report_evaluation expects the actual labels to be a list of 0 and 1, this is neccessary:\n",
    "test.loc[test['label'] == 'LABEL_0', 'label'] = 0\n",
    "test.loc[test['label'] == 'LABEL_1', 'label'] = 1\n",
    "\n",
    "# Get scores/results\n",
    "report_evaluation(nb_bert_results['label'], list(test['label']), 'nb-bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4f7c9",
   "metadata": {},
   "source": [
    "## NorBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d379ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/NorBERT-original')\n",
    "norbert_train_encoding = train_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "norbert_eval_encoding = eval_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Train model for 5 epochs\n",
    "fine_tuned_norbert_dir, info_from_norbert_training = create_and_train_model(\n",
    "    model_path='./models/NorBERT-original',\n",
    "    model_name='norbert',\n",
    "    training_data=norbert_train_encoding,\n",
    "    eval_data=norbert_eval_encoding,\n",
    "    epochs=5,\n",
    "    learning_rate=5e-05,\n",
    "    warmup_ratio=0,\n",
    "    optimizer='adamw_hf',\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "print(f'info_from_norbert_training: {info_from_norbert_training}')\n",
    "\n",
    "# Make predictions and check prediction time\n",
    "start_time = time.time()\n",
    "norbert_results = predict_from_fine_tuned_model(fine_tuned_norbert_dir, list(test['text']))\n",
    "print(f\"Time used for prediction was: {time.time() - start_time}\")\n",
    "\n",
    "# Because report_evaluation expects the actual labels to be a list of 0 and 1, this is neccessary:\n",
    "test.loc[test['label'] == 'LABEL_0', 'label'] = 0\n",
    "test.loc[test['label'] == 'LABEL_1', 'label'] = 1\n",
    "\n",
    "# Get scores/results\n",
    "report_evaluation(norbert_results['label'], list(test['label']), 'norbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e64a17",
   "metadata": {},
   "source": [
    "## mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/mBERT-original')\n",
    "mbert_train_encoding = train_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "mbert_eval_encoding = eval_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Train model for 5 epochs\n",
    "fine_tuned_mbert_dir, info_from_mbert_training = create_and_train_model(\n",
    "    model_path='./models/mBERT-original',\n",
    "    model_name='mbert',\n",
    "    training_data=mbert_train_encoding,\n",
    "    eval_data=mbert_eval_encoding,\n",
    "    epochs=5,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_ratio=0.1,\n",
    "    optimizer='adamw_hf',\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "print(f'info_from_mbert_training: {info_from_mbert_training}')\n",
    "\n",
    "# Make predictions and check prediction time\n",
    "start_time = time.time()\n",
    "mbert_results = predict_from_fine_tuned_model(fine_tuned_mbert_dir, list(test['text']))\n",
    "print(f\"Time used for prediction was: {time.time() - start_time}\")\n",
    "\n",
    "# Because report_evaluation expects the actual labels to be a list of 0 and 1, this is neccessary:\n",
    "test.loc[test['label'] == 'LABEL_0', 'label'] = 0\n",
    "test.loc[test['label'] == 'LABEL_1', 'label'] = 1\n",
    "\n",
    "# Get scores/results\n",
    "report_evaluation(mbert_results['label'], list(test['label']), 'mbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb531270",
   "metadata": {},
   "source": [
    "## Save the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilmbert_results.to_csv('./scores/model_predictions/distilmbert_results.csv', index=False)\n",
    "nb_bert_results.to_csv('./scores/model_predictions/nb_bert_results.csv', index=False)\n",
    "norbert_results.to_csv('./scores/model_predictions/norbert_results.csv', index=False)\n",
    "mbert_results.to_csv('./scores/model_predictions/mbert_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e6327",
   "metadata": {},
   "source": [
    "# Test the best model on a new dataset\n",
    "\n",
    "Now the best model (NorBERT) will be tested on a new dataset, and the results from that dataset will be handed to employees at SMN. They will give qualitative feedback on wether or not the results from a language model can be useful for prioritizing incoming emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39c9ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_df = pd.read_csv('./preprocessed_emails_with_all_columns.csv')\n",
    "norbert_results_on_small_df = predict_from_fine_tuned_model('./models/norbert_LR5e-05_WR0_OPTIMadamw_hf_WD0.01', list(small_df['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edede18",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df['label'] = list(norbert_results_on_small_df['label'])\n",
    "small_df['score'] = list(norbert_results_on_small_df['score'])\n",
    "small_df.to_csv('./new_small_email_dataset_with_predictions_all_columns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2753117",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a980719",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "norbert_results_on_small_df.value_counts('label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
